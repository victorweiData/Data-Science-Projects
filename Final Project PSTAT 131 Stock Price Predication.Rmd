---
title: "Final_Project"
author: "Victor Wei"
date: '2022-03-12'
output: html_document
---
```{r setup, include=FALSE}
#Set up
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ISLR)
library(glmnet)
library(dplyr)
library(tidyr)
library(MASS)
library(leaps)
library(caret)
library(ISLR2)
library(tree)
library(maptree)
library(randomForest)
library(gbm)
library (BART)
library(ggplot2)
library(units)
library(data.table)
library(purrr)
library(rsample)
library(ranger)
data1 <- read_csv("C:/F131/ProjectData.csv")
data1 <- na.omit(data1)
summary(data1)
data_no_date <- subset(data1, select = -c(1))
```
# Intro

## What is the stock market and why do we care?

According to Investopedia, “the stock market broadly refers to the collection of exchanges and other venues where the buying, selling, and issuance of shares of publicly held companies take place. Such financial activities are conducted through institutionalized formal exchanges (whether physical or electronic) or via over-the-counter (OTC) marketplaces that operate under a defined set of regulations.”

Stock markets are important, especially in a free-market economy like that of the United States, because they allow any investor to trade and exchange capital. Through the stock market, companies can issue and sell their shares to raise capital so that they can carry out their business activities.

To be able to gauge how the market is doing, there exist many indices that aim to provide a measure in order to track the movement of the market. One such index is the S&P 500 index (which stands for Standard & Poor’s). The S&P 500 index is composed of a list of the 500 largest companies in the U.S. across 11 sectors of businesses. The calculation of the index weighs each company based on its market capitalization (which includes the company's outstanding shares and current share price).  

For the purposes of this project, we will focus on SPX and how various monetary policies such as the interest rate, M2 real money supply, Federal Funds Effective Rate, unemployment rate, CPI, 10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity, and 30-Year Fixed Rate Mortgage Average in the United States affect SPX’s price. These are the 7 variables that we will use to predict SPX price in the future.

## What models are we going to use?

In this project, we will use several different regression models to predict our SPX outcome. This includes a linear regression model, a ridge regression model, a lasso regression model, a regression tree, the  method, and the boosting method to predict SPX using our 7 variables. Some of these models might not work as well as desired due to the fact that we don't have too many predictors and they can be correlated to some degree, but we will use methods like step-wise selection and cross-validation in order to enhance the models' performances. We will, then, compare the performances of our models and find the most accurate one to serve as our final model.

## Why might these models be useful?

These models might be useful because it can be applied in real life. Specifically, should it succeed in predicting the SPX values somewhat accurately, even though it can be very difficult to achieve, we can use it to potentially make money off of our prediction by longing or shorting the SPX. 


## Loading data and packages:

Before diving into our model, we think that it’s important to understand the basics of what each of our predictors mean.

#### From Investopedia:

##### M2 Real Money Supply

A measure of the money supply that includes cash, checking deposits, and easily-convertible near money.

##### Federal Funds Effective Rate

The interest rate banks charge each other for overnight loans to meet their reserve requirements.

##### Unemployment rate

The number of unemployed people as a percentage of the labor force.

##### CPI

The average change in prices over time that consumers pay for a basket of goods and services.

##### 10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity

Constant maturity is the theoretical value of a U.S. Treasury that is based on recent values of auctioned U.S. Treasuries. They are often used by lenders to determine mortgage rates.

##### 30-Year Fixed Rate Mortgage Average

Average of the 30-year fixed-rate mortgage, which is basically a home loan that gives you 30 years to pay back the money you borrowed at an interest rate that won’t change.

##### PCE

Personal consumer expenditures, which measures the change in goods and services consumed by all households, and nonprofit institutions serving households.
# Set Up

## Training/Testing split

```{r}
#Generate x and y
x <- model.matrix(Price~.-Date,data1)
y <- data1$Price
set.seed(123)
#Split data
train <- sample(1:nrow(x),600)
#Training data
x.train <- x[train,]
y.train <- y[train]
#Testing data
x.test <- x[-train,]
y.test <- y[-train]
```

#Part 1, Linear Regression

## Linear Regression on Training Data 

Let's first run a linear regression on our training data:
```{r}
train_lm_fit <- lm(y.train ~ x.train, data = data1)
summary(train_lm_fit)
```
The R^2 value is 0.9521, and the adjusted R^2 is 0.9516, both of which indicates that the model is of good fit, but the summary report also indicates that CPI and Mortgage30 are not very significant. Let's follow up by using some other tools to analyze our data.

## Exploratory Data Analysis
We will be doing an exploratory data analysis on the training data.

## Residuals vs Fitted Plot
Let's run a residuals vs fitted plot on our training data to see if we have constant variance. 
```{r}
plot(fitted(train_lm_fit), residuals(train_lm_fit), xlab = "Fitted", ylab = "Residuals")
abline(0,0)
```
Here, we see that the regression line seems to systematically over and under predict the data at different points along the curve. The data points are not randomly nor evenly spread across the line, indicating that we don't have constant variance in terms of a linear regression. As a result, linear regression appears to not be a great fit for our model.

## QQPlot
Let's see if our datasets come from populations with a common distribution by using the QQPlot:
```{r}
qqnorm(residuals(train_lm_fit))
qqline(residuals(train_lm_fit))
```
Looking at our qqplot, we see that the majority of points fall pretty close along the straight line. This is a good sign, showing that our datasets do seem to come from populations with a common distribution. In this case, we have a normal distribution, since we used the ""qqnorm" function.

However, the points at both ends of the data start wavering and moving away from the line indicating normality. As a result, we can understand that our data is not precisely normal, but it is not too far off as well.

## Cook's Distance
Cook's Distance is used to find any influential outliers in a set of predictors variables. It suggests that any data points that exceed the 4/n threshold (where n = # data points) have strong influence over the fitted values and are considered to be outliers. 

Lets see if we have any particularly influential outliers:
```{r}
plot(cooks.distance(train_lm_fit), pch = 16, col = "blue")
n <- nrow(data1)
abline(h = 4/n, lty = 2, col = "steelblue") # add cutoff line at 4/n
```
Using Cook's Distance to analyze our data, we see that there are many points that cross this 4/n threshold. This doesn't necessarily mean that we should remove all these points. Instead, the presence of so many highly influential points could indicate that a linear regression is not the best model for our data.


## Training MSE
```{r}
# Use the SLR model to predict the values of Price
y_hat_train <- predict(train_lm_fit, data = data1)

training_MSE <- mean((y.train - y_hat_train)^2)
training_MSE
```
The training MSE is 43952.57, which is very large. Let's see if the models we use later give us a better MSE. But first, we should take a look at whether this can be fixed by simply performing a variable selection.

Now that we have analyzed our data, let's perform forward selection to choose our best linear regression model. 

## Forward Selection for Training Data
```{r}
# find cross validation error for a given model
set.seed(123)

# Use 10-fold cross-validation to estimate the average prediction error AKA RMSE of each of the 7 possible models
train_control <- trainControl(method = "cv", number = 10) # train the data
cv <- train(Price ~ ., data = data1, method = "leapForward", # Forward selection using "leapForward"
            tuneGrid = data.frame(nvmax = 1:7), #nvmax stands for the number of variables in the model.
            trControl = train_control
)
cv$results
cv$bestTune
```

The lower the RMSE and MAE are, the better the model. From the results of our forward selection, it seems that the 7-variable model is the best out of all 7 types of models. It has the lowest RMSE and MAE along with the highest R^2 value. However, these values for 4, 5, 6, and 7 predictor values are all very close. Additionally, cv$bestTune is also telling use that the best model seems to be the one with 7 predictor variables.

As a result, for us to produce the most accurate results with a linear regression on our data, using all 7 predictors will yield the best results. 


## Linear Regression on Testing Data
Now, let's move on to performing linear regression on our testing data.
```{r}
test_lm_fit <- lm(y.test ~ x.test, data = data1)
summary(test_lm_fit)
# R^2 is 0.9527
plot(fitted(test_lm_fit), residuals(test_lm_fit), xlab = "Fitted", ylab = "Residuals")
abline(0,0)

```

R^2 is 0.9527, but the residuals vs fitted graph shows that the regression line systematically over/under predicts the data at different points along the curve. Linear regression appears to not be a good fit for our model.


## Testing MSE:
```{r}
y_hat_test <- predict(test_lm_fit, data = data1)

testing_MSE <- mean((y.test - y_hat_test)^2)
testing_MSE
```

The testing MSE is 40920.33, which is slightly smaller than training MSE, which is not normal. Also, it is still huge, which implies an underfitted model. Let's see if variable selection could improve it.

## Forward Selection for Testing Data

To perform forward selection on our testing data, we are going to use the "regsubsets" function, which performs best subset selection by identifying the best model when given a number of predictors based on RSS (residual sum of squares). By setting "nvmax = 7", we are saying that we want to test what the best model is for 1 predictor, 2 predictors, etc., all the way up to 7 predictors.
```{r}
model_options <- regsubsets(Price ~.-Date, data = data1[-train,], nvmax = 7,
                     method = "forward")
summary_best_subset <- summary(model_options) 
summary_best_subset
which.max(summary_best_subset$adjr2) # to find number of predictors that gives us the best model
```
Performing forward selection on our test data, we also see that using all 7 predictors gives us the best model under linear regression. This would make sense, as our R^2 value for the linear regression on all 7 predictors was already quite high. 

In conclusion, from performing linear regression, we found that keeping all 7 of our predictor variables will give us the best predictions of stock market price. However, this is only the best model under linear regression, which as we saw from our data analysis, does not seem to be the model that best fits our data. Now, lets move into another model to see if it better fits and predicts our data. 

# Part 2, Ridge and Lasso

## Generate ridge model

```{r}
#Generate grid
grid <- 10^seq(10, -2, length = 100)
#Ridge regression
ridge_mod <- glmnet(x.train, y.train, alpha = 0, lambda = grid)
ridge_mod
```
## Perform cross-validation

```{r}
#Cross-validation
cvmodr <- cv.glmnet(x.train,y.train,alpha=0,folds=10)
cvmodr
#Get the best lambda, according to CV
ridge_lambda <- cvmodr$lambda.min
ridge_lambda
#Predict based on the best lambda
predict(ridge_mod,type="coefficients",s=ridge_lambda)
```
## The training MSE is:
```{r}
set.seed(123)
ridge.predtr <- predict(ridge_mod,s=ridge_lambda,newx=x.train)
mean((ridge.predtr-y.train)^2)
```
## The testing MSE is:
```{r}
set.seed(123)
ridge.predte <- predict(ridge_mod,s=ridge_lambda,newx=x.test)
mean((ridge.predte-y.test)^2)
```
Both the training and testing MSE are extremely high, and the training MSE is higher than the testing MSE, which is not normal in theory. Therefore, it can be concluded that the model does not fit the data well.

## Generate lasso model

```{r}
set.seed(123)
lasso_mod <- glmnet(x.train,y.train,alpha=1,lambda=grid)
lasso_mod
```
## Perform cross-validation

```{r}
cvmodl <- cv.glmnet(x.train,y.train,alpha=1,folds=10)
cvmodl
```
```{r}
lasso_lambda <- cvmodl$lambda.min
lasso_lambda
predict(lasso_mod,type="coefficients",s=lasso_lambda)
```
Mortgage30 was ruled out

## Training MSE is:
```{r}
set.seed(123)
lasso.predtr <- predict(lasso_mod,s=lasso_lambda,newx=x.train)
mean((lasso.predtr-y.train)^2)
```
## Testing MSE is:
```{r}
set.seed(123)
lasso.predte <- predict(lasso_mod,s=lasso_lambda,newx=x.test)
mean((lasso.predte-y.test)^2)
```
After the lasso regression performed variable selection automatically, dropping Mortgage30 out of the regression model, we get a significantly lower MSE for both the training set and the testing set. However, they are both still really high and the training MSE is still higher than the testing MSE, implying that the model still underfits. 

# Part 3, Tree Regression

```{r}
tree.data_no_date = tree(Price ~ . , data = data_no_date)

plot(tree.data_no_date)
text(tree.data_no_date, pretty = 0, col = "red", cex = 0.6)
title("decision tree on SP 500 price", cex = 0.7)
```

PCE is the variable of first split.

```{r}
# Set random seed for results being reproducible
set.seed(123)
# Sample 75% of observations as the training set
train_tree = sample(nrow(data_no_date), 0.75*nrow(data_no_date))
data_no_date.train = data_no_date[train_tree, ]
# The rest 25% as the test set
data_no_date.test = data_no_date[-train_tree,]
# Fit model on training set
tree.data_no_date_train = tree(Price ~ . , data = data_no_date.train)
# Plot the tree
draw.tree(tree.data_no_date_train, nodeinfo=TRUE, cex = 0.55)
```
## Tree Result
```{r}
tree.pred = predict(tree.data_no_date_train, data_no_date.test)

plot (tree.pred, data_no_date.test$Price)
abline (0, 1)

MSE_tree = mean((tree.pred - data_no_date.test$Price)^2)
MSE_tree
sqrt(MSE_tree)
```
### Tree Interpretation

Tree regression model leads to an MSE of 46910.95 and a RMSE of 216.5894. The MSE is extremely high and the RMSE tells use that on average we are 216.5894 away from the best fitted line on the graph which is extremely inaccurate consider that our SPY price is range from 0 to 4000. 

# Pruning Tree
```{r}

cv.data_no_date <- cv.tree(tree.data_no_date, K = 10)
plot (cv.data_no_date$size , cv.data_no_date$dev, type = "b", col = 'red')
best.cv = min(cv.data_no_date$size[cv.data_no_date$dev == min(cv.data_no_date$dev)])
abline(v=best.cv, lty=2)
# Add lines to identify complexity parameter
min.error = which.min(cv.data_no_date$dev) # Get minimum error index
abline(h = cv.data_no_date$dev[min.error],lty = 2)
```

```{r}
pt.data_no_date <- prune.tree(tree.data_no_date, best = best.cv)

plot(pt.data_no_date)
text(pt.data_no_date , pretty = 0, cex = 0.52, col = "blue")
```

## Pruning Tree Result
```{r}
pred.pt.cv = predict(pt.data_no_date, data_no_date.test)

plot (pred.pt.cv, data_no_date.test$Price)
abline (0, 1)

MSE_prune = mean((pred.pt.cv - data_no_date.test$Price)^2)
MSE_prune
sqrt(MSE_prune)
```
### Prune Tree Interpretation

Prune Tree regression model leads to an MSE of 38330.25 and a RMSE of 195.7811. The MSE is extremely high but is 22.38623% better than the tree regression and the RMSE tells use that on average we are 195.7811 away from the best fitted line on the graph which is extremely inaccurate consider that our SPY price is range from 0 to 4000. Thus, we would use this Prune Tree regression over Tree regression.

#  and random forest

```{r}
# Create training (75%) and test (25%) sets for data.
# Use set.seed for reproducibility
set.seed(123)

ames_split <- sample(nrow(data_no_date), 0.75*nrow(data_no_date))
ames_train <- data_no_date[train_tree, ]
ames_test  <- data_no_date[-train_tree,]
```
## Tuning

```{r}
hyper_grid <- expand.grid(
  mtry       = seq(0, 7, by = 1),
  node_size  = seq(1, 9, by = 1),
  sampe_size = c(.75),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)  
  # train model
  model <- ranger(
    formula         = Price ~ ., 
    data            = ames_train, 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)
```

### Best Bagging and randomForest
```{r}
set.seed(123)
bag.data_no_date <- randomForest(
  formula = Price ~ .,
  data    = data_no_date,
  mtry = 3,
  samplesize = 0.75,
  nodesize = 1,
  ntree = 500,
  importance = TRUE
)
```

#### varible importance and error comparsion on best model
```{r}
varImpPlot(bag.data_no_date)

# create training and validation data 
set.seed(123)
valid_split <- initial_split(ames_train, .75)

# training data
ames_train_v2 <- analysis(valid_split)

# validation data
ames_valid <- assessment(valid_split)
x_test <- ames_valid[setdiff(names(ames_valid), "Price")]
y_test <- ames_valid$Price

rf_oob_comp <- randomForest(
  formula = Price ~ .,
  data    = data_no_date,
  mtry = 3,
  samplesize = 0.75,
  nodesize = 1,
  ntree = 500,
  importance = TRUE,
  xtest   = x_test,
  ytest   = y_test
)

# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)

# compare error rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")
```
The first graph shows that variable importance in the bagging. The first is based upon
the mean decrease of accuracy in predictions on the out of bag samples when
a given variable is permuted. The second is a measure of the total decrease
in node impurity that results from splits over that variable, averaged over
all trees.

The second graph is the comparison between OOB error vs Test error with trees ranging from 0 to 2000.

##### Bagging Result
```{r}
bag.pred = predict(bag.data_no_date, data_no_date.test)

plot (bag.pred , data_no_date.test$Price)
abline (0, 1)
```
###### Bagging Interpretation

Gradient Boosting model leads to a RMSE of 41.48185. The RMSE is the lowest compare to all other models and it tells us that on average we are 41.48185 away from the best fitted line on the graph which is pretty decent.

# Boosting
```{r}
set.seed (123)
```
## Tuning
```{r}
hyper_grid <- expand.grid(
  shrinkage = c(.01, .05, .1, .2),
  interaction.depth = c(1, 2, 3, 4),
  n.minobsinnode = c(1, 5, 10, 15),
  bag.fraction = c(.65, .7, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# randomize data
random_index <- sample(nrow(data_no_date), 0.75*nrow(data_no_date))
random_ames_train <- data_no_date[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(
    formula = Price ~ . ,
    distribution = "gaussian",
    data = random_ames_train,
    n.trees = 2000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

### Best boost
```{r}
# for reproducibility
set.seed(123)

# train GBM model
gbm.fit.final <- gbm(
  formula = Price ~ . ,
  distribution = "gaussian",
  data = data_no_date,
  n.trees = 478,
  interaction.depth = 4,
  shrinkage = 0.05	,
  n.minobsinnode = 5,
  bag.fraction = .75, 
  train.fraction = 1,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  
```

#### Variance Importance
```{r}
par(mar = c(5, 8, 1, 1))
summary(
  gbm.fit.final, 
  cBars = 10,
  method = relative.influence,
  las = 2
  )
```

##### Boosting Result
```{r}
boost.pred = predict(gbm.fit.final, data_no_date.test)

plot (boost.pred , data_no_date.test$Price)
abline (0, 1)
```
###### Boosting Interpretation

Gradient Boosting model leads a RMSE of 55.01865. The RMSE is low compare to all other models except bagging and the RMSE tells use that on average we are 55.01865 away from the best fitted line on the graph which is pretty decent. However, we would not use this model over random forest.


# Conclusion

In conclusion, the best model is the bagging/random forest that we generated before. With a RMSE of 41.48185 and a MSE of 1720.74388, it performs the best among all models. Even though this is still not very accurate, we believe that for beginner level stock predicting it performs well enough. Thus, we would use random forest as our model.
